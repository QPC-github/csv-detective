# -*- coding: utf-8 -*-
"""
###############################################################################
Ce script analyse les premières lignes d'un CSV pour essayer de déterminer le
contenu possible des champs


ACTUELLEMENT EN DEVELOPPEMENT : Indactions comment tester tout en bas
"""

import pandas as pd
from os.path import join
import os
import itertools

import detect_fields

from detection import (ints_as_floats,
                       detect_separator,
                       detect_encoding,
                       detect_headers,
                       detect_heading_columns,
                       detect_trailing_columns,
                       )

#############################################################################
############### ROUTINE DE TEST CI DESSOUS ##################################


def test_col(serie, test_func, proportion = 0.9, skipna = True, num_lines = 50):
    ''' Tests values of the serie using test_func.
         - skipna : if True indicates that NaNs are not counted as False
         - proportion :  indicates the proportion of values that have to pass the test
    for the serie to be detected as a certain type
    '''
    serie = serie[serie.notnull()]
    ser_len = len(serie)
    if ser_len == 0:
        return False
    if proportion == 1: # Then try first 1 value, then 5, then all
        for range_ in [range(0,min(1, ser_len)), range(min(1, ser_len),min(5, ser_len)), range(min(5, ser_len),min(num_lines, ser_len))]: # Pour ne pas faire d'opérations inutiles, on commence par 1, puis 5 puis num_lines valeurs
            if all(serie.iloc[range_].apply(test_func)):
                pass
            else:
                return False
        return True
    else:
        return serie.apply(test_func).sum() > proportion * len(serie)


def all_tests_recursive(path):
    '''Returns all the tests that are in path as paths'''
    in_path = os.listdir(path)
    if len(in_path) == 0:
        raise('Path must point to folder')
    elif all([os.path.isfile(os.path.join(path, x)) for x in in_path]):
        return [path]
    else:
        all_folders = [x for x in in_path if os.path.isdir(os.path.join(path, x))]
        all_folder_paths = [os.path.join(path, folder) for folder in all_folders]
        return [os.path.join(x) for x in itertools.chain.from_iterable([all_tests_recursive(path) for path in all_folder_paths])]

def return_all_tests(user_input_tests):
    '''Returns a list of python functions that are the tests to be passed
    user_input_tests can be:
        - None : All tests available will be conducted
        - string ('FR.geo') : Will get all tests inside FR.geo directory
        - list of strings (['FR.geo', 'temp', '-FR.geo.code_departement']) : the minus sign will cut off a branch
    '''
    
    if isinstance(user_input_tests, str):
        assert user_input_tests[0] != '-'
        if user_input_tests == 'ALL':
            paths_to_do = ['detect_fields']
        else:
            paths_to_do = [os.path.join('detect_fields', *user_input_tests.split('.'))]
        paths_to_not_do = []
    elif isinstance(user_input_tests, list):
        tests_to_do = [x for x in user_input_tests if x[0] != '-']
        tests_to_not_do = [x for x in user_input_tests if x[0] == '-']
        if 'ALL' in tests_to_do:
            paths_to_do = ['detect_fields'] + [os.path.join('detect_fields', *x.split('.')) \
                                                for x in tests_to_do if x != 'ALL']
        else:
            paths_to_do = [os.path.join('detect_fields', *x.split('.')) for x in tests_to_do]
        paths_to_not_do = [os.path.join('detect_fields', *x[1:].split('.')) for x in tests_to_not_do]

    all_fields_to_do = ['.'.join(y.split(os.sep)) for y in itertools.chain.from_iterable([all_tests_recursive(x) for x in paths_to_do])] 
    all_fields_to_not_do = ['.'.join(y.split(os.sep)) for y in itertools.chain.from_iterable([all_tests_recursive(x) for x in paths_to_not_do])]
    all_fields = [x for x in all_fields_to_do if x not in all_fields_to_not_do]

    # NB on `eval` : I don't see a real risk here since field is generated by os.path.join. Am I wrong ?
    all_tests = [eval(field) for field in all_fields]
    return all_tests


def routine(file, num_lines = 50, user_input_tests = 'ALL'):
    '''Returns a dict with information about the csv table and possible
    column contents
    '''
    print 'This is tests_to_do', user_input_tests

    sep = detect_separator(file)
    headers_row, headers = detect_headers(file, sep)
    heading_columns = detect_heading_columns(file, sep)
    trailing_columns = detect_trailing_columns(file, sep, heading_columns)
    # print headers_row, heading_columns, trailing_columns
    chardet_res = detect_encoding(file)
    print chardet_res

    for encoding in [chardet_res['encoding'], 'ISO-8859-1', 'utf-8']:
        # TODO : modification systematique
        if 'ISO-8859' in encoding:
            encoding = 'ISO-8859-1'

        try:
            file.seek(0)
            table = pd.read_csv(file, sep = sep,
                                skiprows = headers_row,
                                nrows = num_lines, dtype = 'unicode',
                                encoding = encoding
                                )
            break
        except:
            print 'Trying encoding : {encoding}'.format(encoding = encoding)
    else:
#        print '  >> encoding not found'
        return False


    # Detects columns that are ints but written as floats
    res_ints_as_floats = list(ints_as_floats(table))

    # Creating return dictionnary
    return_dict = dict()
    return_dict['encoding'] = encoding
    return_dict['separator'] = sep
    return_dict['headers_row'] = headers_row
    return_dict['headers'] = headers.decode(encoding).encode('utf-8')
    return_dict['heading_columns'] = heading_columns
    return_dict['trailing_columns'] = trailing_columns
    return_dict['ints_as_floats'] = res_ints_as_floats

    all_tests = return_all_tests(user_input_tests)

    # Initialising dict for tests
    test_funcs = dict()
    for test in all_tests:
        name = test.__name__.split('.')[-1]
        test_funcs[name] = {'func' : test._is,
                            'prop' : test.PROPORTION
                            }

    return_table = pd.DataFrame(columns = table.columns)
    for key, value in test_funcs.iteritems():
        return_table.loc[key] = table.apply(lambda serie: test_col(serie, value['func'], value['prop']))

    # Filling the columns attributes of return dictionnary
    return_dict_cols = dict()
    for col in return_table.columns:
        possible_values = list(return_table[return_table[col]].index)
        if possible_values != []:
            print '  >>  La colonne', col, 'est peut-être :',
            print possible_values
            return_dict_cols[col] = possible_values
    return_dict['columns'] = return_dict_cols
    return return_dict



if __name__ == '__main__':

    # Import the csv_detective package
    #from csv_detective.explore_csv import routine

    # Replace by your file path
    import os
    import json

    file_path = os.path.join('..', 'tests', 'code_postaux_v201410.csv')

    list_tests = ['ALL', '-FR.geo.code_departement']
    # Open your file and run csv_detective
    with open(file_path, 'r') as file:
        inspection_results = routine(file, user_input_tests = list_tests)

    # Write your file as json
    with open(file_path.replace('.csv', '.json'), 'wb') as fp:
        json.dump(inspection_results, fp, indent=4, separators=(',', ': '), encoding="utf-8")

    import pdb
    pdb.set_trace()
    assert False

    from os import listdir
    from os.path import isfile, join
    import json

    ### CONSIGNES : Mettre toutes les data a tester dans le dossier indiqué par path
    # et lancer le script. Il doit afficherc pour chaque fichier dans ce dossier (ne doit contenir que des csv)
    # les colonnes pour lesquelles un match a été trouvé

    # main_path = '/home/debian/Documents/'
    main_path = 'C:/git/csv_detective/'
    path = main_path + 'data/test_csv_detector' #
    json_path = main_path + 'data/test_csv_detector/jsons'

    num_lines = 50 # nombre de lignes à analyser

    all_files = listdir(path)
    counter = 0
    for file_name in all_files:
        print '*****************************************'
        print file_name
        if any([extension in file_name for extension in ['.csv', '.tsv']]):
            file = open(join(path, file_name), 'r')
            a = routine(file)
            file.close()
            if a:
                counter += len(a)
                with open(join(json_path, file_name.replace('.csv', '.json')), 'wb') as fp:
                    json.dump(a, fp, indent=4, separators=(',', ': '), encoding="utf-8")
        print '\n'
    print 'on a trouvé des matchs éventuels pour ', counter, 'valeurs'


